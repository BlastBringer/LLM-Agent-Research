{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":91721,"databundleVersionId":13760552,"sourceType":"competition"}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install required packages (some may already be pre-installed in Kaggle)\n!pip install -q datasets transformers torch tqdm accelerate bitsandbytes\n\nimport os\nimport json\nimport time\nimport re\nfrom tqdm import tqdm\nimport torch\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# Create output directory\nos.makedirs(\"/kaggle/working/results\", exist_ok=True)\n\n# ---------- Utility functions ----------\ndef normalize_answer(ans: str) -> str:\n    return ans.strip().lower().replace(\",\", \"\")\n\ndef is_answer_correct(pred, gold):\n    \"\"\"Compare numeric answers, ignoring formatting and symbols like % or commas.\"\"\"\n    pred_nums = re.findall(r\"[-+]?\\d*\\.?\\d+\", pred)\n    gold_nums = re.findall(r\"[-+]?\\d*\\.?\\d+\", gold)\n\n    if not pred_nums or not gold_nums:\n        return pred.strip().lower() == gold.strip().lower()\n\n    try:\n        pred_val = float(pred_nums[-1])\n        gold_val = float(gold_nums[-1])\n        return abs(pred_val - gold_val) < 1e-3\n    except ValueError:\n        return False\n\n# ---------- Benchmark class ----------\nclass PhiBenchmark:\n    def __init__(self, model_name=\"microsoft/Phi-3.5-mini-instruct\",\n                 device=None, max_new_tokens=512, temperature=0.0):\n        print(f\"ðŸ”¹ Loading model: {model_name}\")\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(\n            model_name, torch_dtype=torch.float16, device_map=\"auto\"\n        )\n        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.max_new_tokens = max_new_tokens\n        self.temperature = temperature\n\n    def make_prompt(self, question):\n        return (\n            \"You are a math reasoning assistant. \"\n            \"Solve step by step and finish with 'Answer: <number>'.\\n\\n\"\n            f\"Problem: {question}\\n\\nSolution:\\n\"\n        )\n\n    def extract_answer(self, text):\n        text = re.split(r\"\\b(Problem:|Question:)\\b\", text)[0]\n        if \"Answer:\" in text:\n            ans = text.split(\"Answer:\")[-1].strip()\n            return ans.split(\"\\n\")[0]\n        nums = re.findall(r\"[-+]?\\d*\\.?\\d+\", text)\n        return nums[-1].strip() if nums else text.strip().split(\"\\n\")[-1]\n\n    @torch.inference_mode()\n    def run_one(self, question):\n        prompt = self.make_prompt(question)\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n        start = time.time()\n        output = self.model.generate(\n            **inputs,\n            max_new_tokens=self.max_new_tokens,\n            temperature=self.temperature,\n            do_sample=False,\n            pad_token_id=self.tokenizer.eos_token_id,\n        )\n        latency = time.time() - start\n        decoded = self.tokenizer.decode(output[0], skip_special_tokens=True)\n        pred = self.extract_answer(decoded)\n\n        # Retry if junk output\n        if len(pred) < 2 or pred in {\"'\", '\"', \"'.\", \".\", \":\", \";\"}:\n            output = self.model.generate(\n                **inputs,\n                max_new_tokens=self.max_new_tokens,\n                temperature=0.7,\n                do_sample=True,\n                top_p=0.9,\n                pad_token_id=self.tokenizer.eos_token_id,\n            )\n            decoded = self.tokenizer.decode(output[0], skip_special_tokens=True)\n            pred = self.extract_answer(decoded)\n\n        return decoded, pred, latency\n\n    def benchmark(self, dataset, save_path, q_key=\"question\", a_key=\"answer\", num_examples=None, checkpoint_interval=50):\n        correct, latencies = 0, []\n        total_examples = num_examples or len(dataset)\n        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n\n        # Resume support\n        processed = set()\n        if os.path.exists(save_path):\n            with open(save_path, \"r\", encoding=\"utf-8\") as f:\n                for line in f:\n                    try:\n                        data = json.loads(line)\n                        processed.add(data[\"index\"])\n                    except:\n                        continue\n            print(f\"ðŸ”„ Resuming from {len(processed)} completed examples...\")\n\n        with open(save_path, \"a\", encoding=\"utf-8\") as f:\n            for i, ex in enumerate(tqdm(dataset, desc=f\"Running on {save_path}\")):\n                if i in processed:\n                    continue\n                if num_examples and i >= num_examples:\n                    break\n\n                q, gold = ex[q_key], str(ex[a_key])\n                decoded, pred, t = self.run_one(q)\n                ok = is_answer_correct(pred, gold)\n\n                result = {\n                    \"index\": i,\n                    \"question\": q,\n                    \"gold_answer\": gold,\n                    \"prediction\": pred,\n                    \"correct\": ok,\n                    \"latency\": t,\n                }\n\n                f.write(json.dumps(result) + \"\\n\")\n                f.flush()\n                os.fsync(f.fileno())\n\n                # Print every checkpoint_interval examples\n                if i % checkpoint_interval == 0:\n                    print(json.dumps(result, indent=2))\n\n                correct += ok\n                latencies.append(t)\n\n        total = len(latencies)\n        acc = round(100 * correct / total, 2) if total else 0\n        avg_t = round(sum(latencies) / total, 3) if total else 0\n        print(f\"\\nâœ… {save_path}: {correct}/{total} correct â†’ {acc}% | avg latency {avg_t}s\")\n\n        return {\"total\": total, \"correct\": correct, \"accuracy\": acc, \"avg_latency\": avg_t}\n\n\n# ---------- Load GSM-Symbolic dataset ----------\ngsm_symbolic = load_dataset(\"apple/GSM-Symbolic\", split=\"test\")\n\n# ---------- Run benchmark ----------\nbench = PhiBenchmark(\"microsoft/Phi-3.5-mini-instruct\")\n\ngsm_metrics = bench.benchmark(\n    gsm_symbolic,\n    \"/kaggle/working/results/phib_gsm_symbolic_checkpointed_2.jsonl\",\n    q_key=\"question\",\n    a_key=\"answer\",\n    checkpoint_interval=200\n)\n\n# ---------- Save summary ----------\nsummary = {\"gsm_symbolic\": gsm_metrics}\nwith open(\"/kaggle/working/results/summary_gsm.json\", \"w\") as f:\n    json.dump(summary, f, indent=2)\n\nprint(\"\\nðŸ“‚ All results stored in /kaggle/working/results/\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r results.zip /kaggle/working/\nfrom IPython.display import FileLink\nFileLink('results.zip')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}